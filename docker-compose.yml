name: "acu-infra"
services:
  # stream kafka used for ingesting data from producers
  kafka-in:
    container_name: "kafka-in"
    hostname: "kafka-in"
    image: "apache/kafka:3.7.0"
    ports:
      - 9092:9092
    healthcheck:
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://localhost:9092,PLAINTEXT://kafka-in:19092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-in:29093'
      KAFKA_LISTENERS: 'CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: '4L6g3nShT-eMCtK--X86sw'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
  kafka-out:
    container_name: "kafka-out"
    hostname: "kafka-out"
    image: "apache/kafka:3.7.0"
    ports:
      - 9093:9092
    healthcheck:
      test: nc -z localhost 9092 || exit -1
      start_period: 15s
      interval: 5s
      timeout: 10s
      retries: 10
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://localhost:9092,PLAINTEXT://kafka-out:19092'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka-out:29093'
      KAFKA_LISTENERS: 'CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      CLUSTER_ID: '6L6g3nShT-eMCtK--X86sw'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'

  # distributed storage
  hdfs-namenode:
    container_name: "hdfs-namenode"
    image: "apache/hadoop:3"
    hostname: "hdfs-namenode"
    command: ["hdfs", "namenode"]
    ports:
      - "8020:8020"
    env_file:
      - ./hdfs-config/config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  hdfs-datanode:
    depends_on:
      hdfs-namenode:
        condition: service_started
    container_name: "hdfs-datanode"
    image: "apache/hadoop:3"
    hostname: "hdfs-datanode"
    command: ["hdfs", "datanode"]
    env_file:
      - ./hdfs-config/config

  # the kafka connector that sinks the data of 
  # kafka-in into our distributed storage
  kafka-hdfs-sink-connector:
    depends_on:
      kafka-in:
        condition: service_healthy
    container_name: "kafka-hdfs-sink-connector"
    hostname: "kafka-hdfs-sink-connector"
    image: "kafka-connect-hdfs:latest"
    build:
      context: "./kafka-connect-hdfs"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-in:19092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-hdfs-sink-connector"
      CONNECT_REST_PORT: "8083"
      CONNECT_GROUP_ID: "hdfs-sink-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: "hdfs-connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "hdfs-connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "hdfs-connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_TOPIC_CREATION_ENABLE: "true"

  # the kafka connector that sinks the data of 
  # kafka-in into kafka-out by applying a filter.
  # **unused** because Adrien told us it hasn't any sense 
  # kafka-alert-connector:
  #   depends_on:
  #     kafka-in:
  #       condition: service_healthy
  #     kafka-out:
  #       condition: service_healthy
  #   container_name: "kafka-alert-connector"
  #   hostname: "kafka-alert-connector"
  #   image: "kafka-connect-alert:latest"
  #   build:
  #     context: "./kafka-connect-alert"
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-in:19092"
  #     CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-alert-connector"
  #     CONNECT_REST_PORT: "8083"
  #     CONNECT_GROUP_ID: "kafka-alert-cluster"
  #     CONNECT_CONFIG_STORAGE_TOPIC: "alert-connect-configs"
  #     CONNECT_OFFSET_STORAGE_TOPIC: "alert-connect-offsets"
  #     CONNECT_STATUS_STORAGE_TOPIC: "alert-connect-status"
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
  #     CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
  #     CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  #     # CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
  #     CONNECT_TOPIC_CREATION_ENABLE: "true"

  # Cluster spark
  spark-master:
    container_name: "spark-master"
    image: "bitnami/spark:3.5"
    hostname: "spark-master"
    environment:
      SPARK_MODE: "master"
    ports:
      - "8080:8080"
      - "7077:7077"
  spark-worker:
    depends_on:
      spark-master:
        condition: service_started
    image: "bitnami/spark:3.5"
    environment:
      SPARK_MODE: "worker"
      SPARK_MASTER_URL: "spark://spark-master:7077"
      SPARK_WORKER_MEMORY: "1G"
      SPARK_WORKER_CORES: "1"

  # Analytics database
  analytics-db:
    container_name: "analytics-db"
    hostname: "analytics-db"
    image: acu-database:latest
    restart: unless-stopped
    build:
      context: ./database
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: acu_infra
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      DASHBOARD_SERVICE_PASSWORD: dashboard123
      ANALYTICS_SERVICE_PASSWORD: analytics123
